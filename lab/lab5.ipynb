{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3c59b6-c05d-4d5b-a476-f4a72eb35448",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Сюжет «Графа Монте-Кристо» был почерпнут Александром Дюма из архивов парижской полиции. \n",
    "Подлинная жизнь Франсуа Пико под пером блестящего мастера историко-приключенческого жанра превратилась в захватывающую историю \n",
    "об Эдмоне Дантесе, узнике замка Иф. Совершив дерзкий побег, он возвращается в родной город, чтобы свершить правосудие – отомстить тем, \n",
    "кто разрушил его жизнь.'''\n",
    "text2 = '中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于1949年10月1日，首都为北京市。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b5d0da-bb13-4f0d-881d-b179ab2bb952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natasha in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: navec>=0.9.0 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from natasha) (0.10.0)\n",
      "Requirement already satisfied: pymorphy2 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from natasha) (0.9.1)\n",
      "Requirement already satisfied: yargy>=0.14.0 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from natasha) (0.15.0)\n",
      "Requirement already satisfied: slovnet>=0.3.0 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: ipymarkup>=0.8.0 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from natasha) (0.9.0)\n",
      "Requirement already satisfied: razdel>=0.5.0 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: intervaltree>=3 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
      "Requirement already satisfied: numpy in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from navec>=0.9.0->natasha) (1.22.3)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in ./Downloads/anaconda/anaconda3/lib/python3.9/site-packages (from pymorphy2->natasha) (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d08e8f-2fe7-4a26-b2f7-c34dc6de37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize, sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285c6ba1-0298-4a9b-b61b-afb14745f70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 5, 'Сюжет'),\n",
       " Substring(6, 7, '«'),\n",
       " Substring(7, 12, 'Графа'),\n",
       " Substring(13, 25, 'Монте-Кристо'),\n",
       " Substring(25, 26, '»'),\n",
       " Substring(27, 30, 'был'),\n",
       " Substring(31, 40, 'почерпнут'),\n",
       " Substring(41, 52, 'Александром'),\n",
       " Substring(53, 57, 'Дюма'),\n",
       " Substring(58, 60, 'из'),\n",
       " Substring(61, 68, 'архивов'),\n",
       " Substring(69, 78, 'парижской'),\n",
       " Substring(79, 86, 'полиции'),\n",
       " Substring(86, 87, '.'),\n",
       " Substring(89, 98, 'Подлинная'),\n",
       " Substring(99, 104, 'жизнь'),\n",
       " Substring(105, 112, 'Франсуа'),\n",
       " Substring(113, 117, 'Пико'),\n",
       " Substring(118, 121, 'под'),\n",
       " Substring(122, 127, 'пером'),\n",
       " Substring(128, 138, 'блестящего'),\n",
       " Substring(139, 146, 'мастера'),\n",
       " Substring(147, 172, 'историко-приключенческого'),\n",
       " Substring(173, 178, 'жанра'),\n",
       " Substring(179, 191, 'превратилась'),\n",
       " Substring(192, 193, 'в'),\n",
       " Substring(194, 207, 'захватывающую'),\n",
       " Substring(208, 215, 'историю'),\n",
       " Substring(217, 219, 'об'),\n",
       " Substring(220, 226, 'Эдмоне'),\n",
       " Substring(227, 234, 'Дантесе'),\n",
       " Substring(234, 235, ','),\n",
       " Substring(236, 242, 'узнике'),\n",
       " Substring(243, 248, 'замка'),\n",
       " Substring(249, 251, 'Иф'),\n",
       " Substring(251, 252, '.'),\n",
       " Substring(253, 261, 'Совершив'),\n",
       " Substring(262, 269, 'дерзкий'),\n",
       " Substring(270, 275, 'побег'),\n",
       " Substring(275, 276, ','),\n",
       " Substring(277, 279, 'он'),\n",
       " Substring(280, 292, 'возвращается'),\n",
       " Substring(293, 294, 'в'),\n",
       " Substring(295, 301, 'родной'),\n",
       " Substring(302, 307, 'город'),\n",
       " Substring(307, 308, ','),\n",
       " Substring(309, 314, 'чтобы'),\n",
       " Substring(315, 323, 'свершить'),\n",
       " Substring(324, 334, 'правосудие'),\n",
       " Substring(335, 336, '–'),\n",
       " Substring(337, 346, 'отомстить'),\n",
       " Substring(347, 350, 'тем'),\n",
       " Substring(350, 351, ','),\n",
       " Substring(353, 356, 'кто'),\n",
       " Substring(357, 365, 'разрушил'),\n",
       " Substring(366, 369, 'его'),\n",
       " Substring(370, 375, 'жизнь'),\n",
       " Substring(375, 376, '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tok_text = list(tokenize(text))\n",
    "n_tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82da2e0e-c091-4740-a914-6ad818280f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сюжет',\n",
       " '«',\n",
       " 'Графа',\n",
       " 'Монте-Кристо',\n",
       " '»',\n",
       " 'был',\n",
       " 'почерпнут',\n",
       " 'Александром',\n",
       " 'Дюма',\n",
       " 'из',\n",
       " 'архивов',\n",
       " 'парижской',\n",
       " 'полиции',\n",
       " '.',\n",
       " 'Подлинная',\n",
       " 'жизнь',\n",
       " 'Франсуа',\n",
       " 'Пико',\n",
       " 'под',\n",
       " 'пером',\n",
       " 'блестящего',\n",
       " 'мастера',\n",
       " 'историко-приключенческого',\n",
       " 'жанра',\n",
       " 'превратилась',\n",
       " 'в',\n",
       " 'захватывающую',\n",
       " 'историю',\n",
       " 'об',\n",
       " 'Эдмоне',\n",
       " 'Дантесе',\n",
       " ',',\n",
       " 'узнике',\n",
       " 'замка',\n",
       " 'Иф',\n",
       " '.',\n",
       " 'Совершив',\n",
       " 'дерзкий',\n",
       " 'побег',\n",
       " ',',\n",
       " 'он',\n",
       " 'возвращается',\n",
       " 'в',\n",
       " 'родной',\n",
       " 'город',\n",
       " ',',\n",
       " 'чтобы',\n",
       " 'свершить',\n",
       " 'правосудие',\n",
       " '–',\n",
       " 'отомстить',\n",
       " 'тем',\n",
       " ',',\n",
       " 'кто',\n",
       " 'разрушил',\n",
       " 'его',\n",
       " 'жизнь',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in n_tok_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0deb32-1c71-4dca-b76d-4d3ade747022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           87,\n",
       "           'Сюжет «Графа Монте-Кристо» был почерпнут Александром Дюма из архивов парижской полиции.'),\n",
       " Substring(89,\n",
       "           252,\n",
       "           'Подлинная жизнь Франсуа Пико под пером блестящего мастера историко-приключенческого жанра превратилась в захватывающую историю \\nоб Эдмоне Дантесе, узнике замка Иф.'),\n",
       " Substring(253,\n",
       "           376,\n",
       "           'Совершив дерзкий побег, он возвращается в родной город, чтобы свершить правосудие – отомстить тем, \\nкто разрушил его жизнь.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_text = list(sentenize(text))\n",
    "n_sen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25530d21-707a-438f-87cc-2d2bc257d66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Сюжет «Графа Монте-Кристо» был почерпнут Александром Дюма из архивов парижской полиции.',\n",
       "  'Подлинная жизнь Франсуа Пико под пером блестящего мастера историко-приключенческого жанра превратилась в захватывающую историю \\nоб Эдмоне Дантесе, узнике замка Иф.',\n",
       "  'Совершив дерзкий побег, он возвращается в родной город, чтобы свершить правосудие – отомстить тем, \\nкто разрушил его жизнь.'],\n",
       " 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in n_sen_text], len([_.text for _ in n_sen_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ecb23e2-5d66-4a1d-920c-97e7fde8babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот вариант токенизации нужен для последующей обработки\n",
    "def n_sentenize(text):\n",
    "    n_sen_chunk = []\n",
    "    for sent in sentenize(text):\n",
    "        tokens = [_.text for _ in tokenize(sent.text)]\n",
    "        n_sen_chunk.append(tokens)\n",
    "    return n_sen_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9e58df-3556-4a03-b488-96b2ad557707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Сюжет',\n",
       "  '«',\n",
       "  'Графа',\n",
       "  'Монте-Кристо',\n",
       "  '»',\n",
       "  'был',\n",
       "  'почерпнут',\n",
       "  'Александром',\n",
       "  'Дюма',\n",
       "  'из',\n",
       "  'архивов',\n",
       "  'парижской',\n",
       "  'полиции',\n",
       "  '.'],\n",
       " ['Подлинная',\n",
       "  'жизнь',\n",
       "  'Франсуа',\n",
       "  'Пико',\n",
       "  'под',\n",
       "  'пером',\n",
       "  'блестящего',\n",
       "  'мастера',\n",
       "  'историко-приключенческого',\n",
       "  'жанра',\n",
       "  'превратилась',\n",
       "  'в',\n",
       "  'захватывающую',\n",
       "  'историю',\n",
       "  'об',\n",
       "  'Эдмоне',\n",
       "  'Дантесе',\n",
       "  ',',\n",
       "  'узнике',\n",
       "  'замка',\n",
       "  'Иф',\n",
       "  '.'],\n",
       " ['Совершив',\n",
       "  'дерзкий',\n",
       "  'побег',\n",
       "  ',',\n",
       "  'он',\n",
       "  'возвращается',\n",
       "  'в',\n",
       "  'родной',\n",
       "  'город',\n",
       "  ',',\n",
       "  'чтобы',\n",
       "  'свершить',\n",
       "  'правосудие',\n",
       "  '–',\n",
       "  'отомстить',\n",
       "  'тем',\n",
       "  ',',\n",
       "  'кто',\n",
       "  'разрушил',\n",
       "  'его',\n",
       "  'жизнь',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_chunk = n_sentenize(text)\n",
    "n_sen_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c370979c-4b46-4984-b3d8-05fc27da0129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于', '1949', '年', '10', '月', '1', '日，首都为北京市。']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_chunk_2 = n_sentenize(text2)\n",
    "n_sen_chunk_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78889275-81b2-40bd-ba1a-4d3a3ba40f86",
   "metadata": {},
   "source": [
    "### Частеречная разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0d9aea-9f03-49c7-ad86-7d44f515fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from slovnet import Morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7edd24da-f967-48ad-a45d-ce92cdd8c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл необходимо скачать по ссылке https://github.com/natasha/navec#downloads\n",
    "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8090dd9f-811d-4682-8402-f526e755b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл необходимо скачать по ссылке https://github.com/natasha/slovnet#downloads\n",
    "n_morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364cd5ba-1814-4193-8501-9ba64433be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_res = n_morph.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90b4889d-6188-4cf3-bcce-4cb2dd453980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pos(markup):\n",
    "    for token in markup.tokens:\n",
    "        print('{} - {}'.format(token.text, token.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea10a7f4-1403-4b98-a2d6-e8388c946e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сюжет - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "« - PUNCT\n",
      "Графа - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      "Монте-Кристо - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      "» - PUNCT\n",
      "был - AUX|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\n",
      "почерпнут - VERB|Aspect=Perf|Gender=Masc|Number=Sing|Tense=Past|Variant=Short|VerbForm=Part|Voice=Pass\n",
      "Александром - PROPN|Animacy=Anim|Case=Ins|Gender=Masc|Number=Sing\n",
      "Дюма - PROPN|Animacy=Anim|Case=Ins|Gender=Masc|Number=Sing\n",
      "из - ADP\n",
      "архивов - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Plur\n",
      "парижской - ADJ|Case=Gen|Degree=Pos|Gender=Fem|Number=Sing\n",
      "полиции - NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
      ". - PUNCT\n",
      "Подлинная - ADJ|Case=Nom|Degree=Pos|Gender=Fem|Number=Sing\n",
      "жизнь - NOUN|Animacy=Inan|Case=Nom|Gender=Fem|Number=Sing\n",
      "Франсуа - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      "Пико - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      "под - ADP\n",
      "пером - NOUN|Animacy=Inan|Case=Ins|Gender=Neut|Number=Sing\n",
      "блестящего - ADJ|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing\n",
      "мастера - NOUN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      "историко-приключенческого - ADJ|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing\n",
      "жанра - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
      "превратилась - VERB|Aspect=Perf|Gender=Fem|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Mid\n",
      "в - ADP\n",
      "захватывающую - ADJ|Case=Acc|Degree=Pos|Gender=Fem|Number=Sing\n",
      "историю - NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing\n",
      "об - ADP\n",
      "Эдмоне - PROPN|Animacy=Anim|Case=Loc|Gender=Masc|Number=Sing\n",
      "Дантесе - PROPN|Animacy=Anim|Case=Loc|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "узнике - ADJ|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing\n",
      "замка - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
      "Иф - PROPN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      ". - PUNCT\n",
      "Совершив - VERB|Aspect=Perf|Tense=Past|VerbForm=Conv|Voice=Act\n",
      "дерзкий - ADJ|Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\n",
      "побег - NOUN|Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "он - PRON|Case=Nom|Gender=Masc|Number=Sing|Person=3\n",
      "возвращается - VERB|Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Mid\n",
      "в - ADP\n",
      "родной - ADJ|Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\n",
      "город - NOUN|Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "чтобы - SCONJ\n",
      "свершить - VERB|Aspect=Perf|VerbForm=Inf|Voice=Act\n",
      "правосудие - NOUN|Animacy=Inan|Case=Acc|Gender=Neut|Number=Sing\n",
      "– - PUNCT\n",
      "отомстить - VERB|Aspect=Perf|VerbForm=Inf|Voice=Act\n",
      "тем - DET|Case=Dat|Number=Plur\n",
      ", - PUNCT\n",
      "кто - PRON|Case=Nom\n",
      "разрушил - VERB|Aspect=Perf|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\n",
      "его - DET\n",
      "жизнь - NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing\n",
      ". - PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_text_markup = list(_ for _ in n_morph.map(n_sen_chunk))\n",
    "[print_pos(x) for x in n_text_markup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a42333c2-85fe-4d81-bc34-0c95ca8ef4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于 - PUNCT\n",
      "1949 - NUM\n",
      "年 - PUNCT\n",
      "10 - NUM\n",
      "月 - PUNCT\n",
      "1 - NUM\n",
      "日，首都为北京市。 - PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_text2_markup = list(n_morph.map(n_sen_chunk_2))\n",
    "[print_pos(x) for x in n_text2_markup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711e3a0-2416-4ebd-aea8-590fc23b6f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b69da7d-4c8f-4209-bbd0-3d86ef7f868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d3073db-2871-4564-b29c-46a0dd9ed07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_lemmatize(text):\n",
    "    emb = NewsEmbedding()\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    segmenter = Segmenter()\n",
    "    morph_vocab = MorphVocab()\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba00cd07-05ad-4e71-abec-7e35119027df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Сюжет': 'сюжет',\n",
       " '«': '«',\n",
       " 'Графа': 'граф',\n",
       " 'Монте-Кристо': 'монте-кристо',\n",
       " '»': '»',\n",
       " 'был': 'быть',\n",
       " 'почерпнут': 'почерпнуть',\n",
       " 'Александром': 'александр',\n",
       " 'Дюма': 'дюма',\n",
       " 'из': 'из',\n",
       " 'архивов': 'архив',\n",
       " 'парижской': 'парижский',\n",
       " 'полиции': 'полиция',\n",
       " '.': '.',\n",
       " 'Подлинная': 'подлинный',\n",
       " 'жизнь': 'жизнь',\n",
       " 'Франсуа': 'франсуа',\n",
       " 'Пико': 'пико',\n",
       " 'под': 'под',\n",
       " 'пером': 'перо',\n",
       " 'блестящего': 'блестящий',\n",
       " 'мастера': 'мастер',\n",
       " 'историко-приключенческого': 'историко-приключенческий',\n",
       " 'жанра': 'жанр',\n",
       " 'превратилась': 'превратиться',\n",
       " 'в': 'в',\n",
       " 'захватывающую': 'захватывать',\n",
       " 'историю': 'история',\n",
       " 'об': 'о',\n",
       " 'Эдмоне': 'эдмон',\n",
       " 'Дантесе': 'дантес',\n",
       " ',': ',',\n",
       " 'узнике': 'узник',\n",
       " 'замка': 'замок',\n",
       " 'Иф': 'иф',\n",
       " 'Совершив': 'совершить',\n",
       " 'дерзкий': 'дерзкий',\n",
       " 'побег': 'побег',\n",
       " 'он': 'он',\n",
       " 'возвращается': 'возвращаться',\n",
       " 'родной': 'родной',\n",
       " 'город': 'город',\n",
       " 'чтобы': 'чтобы',\n",
       " 'свершить': 'свершить',\n",
       " 'правосудие': 'правосудие',\n",
       " '–': '–',\n",
       " 'отомстить': 'отомстить',\n",
       " 'тем': 'тот',\n",
       " 'кто': 'кто',\n",
       " 'разрушил': 'разрушить',\n",
       " 'его': 'его'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = n_lemmatize(text)\n",
    "{_.text: _.lemma for _ in n_doc.tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceaaedda-62b2-49aa-9c01-7defb207d798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于': '中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于',\n",
       " '1949': '1949',\n",
       " '年': '年',\n",
       " '10': '10',\n",
       " '月': '月',\n",
       " '1': '1',\n",
       " '日，首都为北京市。': '日，首都为北京市。'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc2 = n_lemmatize(text2)\n",
    "{_.text: _.lemma for _ in n_doc2.tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64e203-d927-4971-b1ba-aafac7c5559c",
   "metadata": {},
   "source": [
    "### Выделение (распознавание) именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b00d028-34ac-4adb-8432-d81ca28850f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slovnet import NER\n",
    "from ipymarkup import show_span_ascii_markup as show_markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78e83c4d-1f6b-4dae-bb4f-6311ab3e1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NER.load('slovnet_ner_news_v1.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5159f2da-46a1-40ec-aff3-8a9947017de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_res = ner.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0535b3d-33b6-4228-8c19-e085c9ba3e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpanMarkup(\n",
       "    text='中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于1949年10月1日，首都为北京市。',\n",
       "    spans=[]\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup_ner = ner(text2)\n",
    "markup_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7067a582-8557-4a30-9296-e40bf28a75cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于1949年10月1日，首都为北京市。\n"
     ]
    }
   ],
   "source": [
    "show_markup(markup_ner.text, markup_ner.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd534b-b833-4e01-b235-b6780ddd6416",
   "metadata": {},
   "source": [
    "### Разбор предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62115fcf-58cc-470e-aab8-09323f242e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import NewsSyntaxParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c968669a-166b-4dc6-a8e2-7285998e1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = NewsEmbedding()\n",
    "syntax_parser = NewsSyntaxParser(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8edd5b52-40bd-4a23-bf1c-1c0ac75bd3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ┌──────► Сюжет        nsubj:pass\n",
      "  │     ┌► «            punct\n",
      "  │ ┌─┌─└─ Графа        \n",
      "  │ │ └──► Монте-Кристо appos\n",
      "  │ └────► »            punct\n",
      "  │     ┌► был          aux:pass\n",
      "┌─└─┌─┌─└─ почерпнут    \n",
      "│   │ └►┌─ Александром  obl:agent\n",
      "│   │   └► Дюма         flat:name\n",
      "│   │   ┌► из           case\n",
      "│   └►┌─└─ архивов      obl\n",
      "│     │ ┌► парижской    amod\n",
      "│     └►└─ полиции      nmod\n",
      "└────────► .            punct\n"
     ]
    }
   ],
   "source": [
    "n_doc.parse_syntax(syntax_parser)\n",
    "n_doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb41f6c8-94fa-4e8d-88f5-a923d9f66dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌► Подлинная                 amod\n",
      "  ┌────►┌─└─ жизнь                     nsubj\n",
      "  │     └►┌─ Франсуа                   nmod\n",
      "  │       └► Пико                      flat:name\n",
      "  │       ┌► под                       case\n",
      "  │ ┌──►┌─└─ пером                     obl\n",
      "  │ │   │ ┌► блестящего                amod\n",
      "  │ │ ┌─└►└─ мастера                   nmod\n",
      "  │ │ │   ┌► историко-приключенческого amod\n",
      "  │ │ └──►└─ жанра                     nmod\n",
      "┌─└─└─┌───── превратилась              \n",
      "│     │ ┌──► в                         case\n",
      "│     │ │ ┌► захватывающую             amod\n",
      "│     └►└─└─ историю                   obl\n",
      "│     │   ┌► об                        case\n",
      "│   ┌─└►┌─└─ Эдмоне                    nmod\n",
      "│   │   └──► Дантесе                   flat:name\n",
      "│   │     ┌► ,                         punct\n",
      "│   └──►┌─└─ узнике                    acl\n",
      "│       └►┌─ замка                     obj\n",
      "│         └► Иф                        appos\n",
      "└──────────► .                         punct\n"
     ]
    }
   ],
   "source": [
    "n_doc.parse_syntax(syntax_parser)\n",
    "n_doc.sents[1].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff09c5a4-371a-4c71-8418-5de43ec70547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ┌──► Совершив     advcl\n",
      "    │ ┌► дерзкий      amod\n",
      "    │ └─ побег        \n",
      "    │    ,            \n",
      "    │ ┌► он           nsubj\n",
      "┌─┌─└─└─ возвращается \n",
      "│ │ ┌──► в            case\n",
      "│ │ │ ┌► родной       amod\n",
      "│ └►└─└─ город        obl\n",
      "│   ┌──► ,            punct\n",
      "│   │ ┌► чтобы        mark\n",
      "│ ┌►└─└─ свершить     acl\n",
      "│ │ └──► правосудие   nsubj\n",
      "│ │   ┌► –            punct\n",
      "│ │ ┌─└─ отомстить    \n",
      "│ └─└──► тем          det\n",
      "│ │ ┌──► ,            punct\n",
      "│ │ │ ┌► кто          nsubj\n",
      "│ └►└─└─ разрушил     acl:relcl\n",
      "│ │   ┌► его          det\n",
      "│ └──►└─ жизнь        obj\n",
      "└──────► .            punct\n"
     ]
    }
   ],
   "source": [
    "n_doc.parse_syntax(syntax_parser)\n",
    "n_doc.sents[2].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5539b52-fc92-4b65-88a8-b6eb43b3082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ┌► 中华人民共和国，简称中国，是一个位于东亚的社会主义国家，成立于 punct\n",
      "┌───┌─┌─└─ 1949                            \n",
      "│   │ └──► 年                               punct\n",
      "│ ┌─└──►┌─ 10                              nummod\n",
      "│ │     └► 月                               punct\n",
      "│ └──────► 1                               nummod\n",
      "└────────► 日，首都为北京市。                       punct\n"
     ]
    }
   ],
   "source": [
    "n_doc2.parse_syntax(syntax_parser)\n",
    "n_doc2.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15970c4f-55d4-4f2d-9679-318b0519312e",
   "metadata": {},
   "source": [
    "### Векторизация текста на основе модели \"мешка слов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0b09662-a6db-4708-8fcb-4cc87cbd504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b961f69e-0ce6-4f27-8f44-d546e5037df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"rec.motorcycles\", \"rec.sport.baseball\", \"sci.electronics\",\"sci.med\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33b54482-a419-4884-a77b-f2faee3c22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bd6498b-8a07-41d0-8044-35d4e4cf0f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 33448\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(data)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f5f2658-ad2d-4ba4-a1a6-46a85ab1976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrmendel=22213\n",
      "unix=31462\n",
      "amherst=5287\n",
      "edu=12444\n",
      "nathaniel=21624\n",
      "mendell=20477\n",
      "subject=29220\n",
      "re=25369\n",
      "bike=6898\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ce98d-0308-4850-956e-722989b6ec97",
   "metadata": {},
   "source": [
    "### Использование класса CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2e49121-c5aa-4779-8d5c-b7ee5fcda2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2380x33448 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 335176 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = vocabVect.transform(data)\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd56a6c7-d869-4b90-a6c9-8734e54ecf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92fe6c9c-0eff-4191-bd4d-798bb8d0e20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33448"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер нулевой строки\n",
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45782420-8557-444e-936e-1d90302bf07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "print([i for i in test_features.todense()[0].getA1() if i>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f63e62b-805b-46e8-9811-c154edc8ad6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '0000000004',\n",
       " '0000000005',\n",
       " '0000000667',\n",
       " '0000001200',\n",
       " '0001',\n",
       " '00014',\n",
       " '0002']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568e602-3d3e-456a-b9a0-fc6801d0c6a1",
   "metadata": {},
   "source": [
    "### Решение задачи анализа тональности текста на основе модели \"мешка слов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12357820-addc-4366-8f31-322a69629503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7357256c-9318-4197-a194-844a69b31e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wanhao/Downloads/anaconda/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/wanhao/Downloads/anaconda/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/wanhao/Downloads/anaconda/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
      "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
      "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
      "                            '0005111312': 11, '0005111312na1em': 12,\n",
      "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
      "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
      "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
      "                            '001813': 24, '002': 25, '002222': 26,\n",
      "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.9382336841146768\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
      "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
      "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
      "                            '0005111312': 11, '0005111312na1em': 12,\n",
      "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
      "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
      "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
      "                            '001813': 24, '002': 25, '002222': 26,\n",
      "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.9453742497059174\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
      "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
      "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
      "                            '0005111312': 11, '0005111312na1em': 12,\n",
      "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
      "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
      "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
      "                            '001813': 24, '002': 25, '002222': 26,\n",
      "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier()\n",
      "Accuracy = 0.6655358653541747\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a40fc-91d2-49c2-8181-b3a0da09be63",
   "metadata": {},
   "source": [
    "### Разделим выборку на обучающую и тестовую и проверим решение для лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44963555-4fbd-4b82-9786-630052857cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups['data'], newsgroups['target'], test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52070c35-a156-456c-b5c6-0c688686654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670c87f-5174-4b6b-8824-07d5769efcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9290322580645162\n",
      "1 \t 0.9675090252707581\n",
      "2 \t 0.9026845637583892\n",
      "3 \t 0.9245901639344263\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132d6db-a745-46db-a3ed-bc1bad4aa550",
   "metadata": {},
   "source": [
    "### Работа с векторными представлениями слов с использованием word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ffcf406-1ecd-43fd-871e-33be49bb8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7a5f3c5-c666-40db-9b57-c4e1d57239d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
       " <http.client.HTTPMessage at 0x7ff2d158cf10>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06adca58-77f3-48bb-aac5-11cce0706b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49de026f-ede2-48fd-9c81-1acf0cbcf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be98d1a6-d26f-4a9a-85cb-4bdb97556d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['холод_S', 'мороз_S', 'береза_S', 'сосна_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd622ad2-5067-4229-bed5-6e854a20bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "СЛОВО - холод_S\n",
      "5 ближайших соседей слова:\n",
      "стужа_S => 0.7676384449005127\n",
      "сырость_S => 0.6338974237442017\n",
      "жара_S => 0.6089427471160889\n",
      "мороз_S => 0.5890367031097412\n",
      "озноб_S => 0.5776054859161377\n",
      "\n",
      "СЛОВО - мороз_S\n",
      "5 ближайших соседей слова:\n",
      "стужа_S => 0.6425478458404541\n",
      "морозец_S => 0.5947279334068298\n",
      "холод_S => 0.5890367031097412\n",
      "жара_S => 0.5522177219390869\n",
      "снегопад_S => 0.5083199143409729\n",
      "\n",
      "СЛОВО - береза_S\n",
      "5 ближайших соседей слова:\n",
      "сосна_S => 0.7943246364593506\n",
      "тополь_S => 0.7562224864959717\n",
      "дуб_S => 0.7440179586410522\n",
      "дерево_S => 0.7373415231704712\n",
      "клен_S => 0.7105199694633484\n",
      "\n",
      "СЛОВО - сосна_S\n",
      "5 ближайших соседей слова:\n",
      "береза_S => 0.7943246960639954\n",
      "дерево_S => 0.758143424987793\n",
      "лиственница_S => 0.7478147745132446\n",
      "дуб_S => 0.7412481307983398\n",
      "ель_S => 0.7363823652267456\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word in model:\n",
    "        print('\\nСЛОВО - {}'.format(word))\n",
    "        print('5 ближайших соседей слова:')\n",
    "        for word, sim in model.most_similar(positive=[word], topn=5):\n",
    "            print('{} => {}'.format(word, sim))\n",
    "    else:\n",
    "        print('Слово \"{}\" не найдено в модели'.format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75c790-123b-47ee-b49f-9db2d423ba1b",
   "metadata": {},
   "source": [
    "### Находим близость между словами и строим аналогии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e09851b4-db66-4bd8-8a46-7d7961ba1c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('сырость_S', 0.5040211081504822), ('стылость_S', 0.46336129307746887), ('голод_S', 0.4604816734790802), ('зной_S', 0.45904627442359924), ('скука_S', 0.4489358067512512), ('жара_S', 0.44645124673843384), ('усталость_S', 0.42185699939727783), ('озноб_S', 0.41469812393188477), ('духота_S', 0.4099087715148926), ('неуют_S', 0.40298792719841003)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['холод_S', 'стужа_S'], negative=['мороз_S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2986ae3-7ca5-4751-82a4-df08fdf7d546",
   "metadata": {},
   "source": [
    "### Обучим word2vec на наборе данных \"fetch_20newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "079d571a-a0da-4fd5-ba82-e787fda30692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wanhao/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3ba84f8-043a-4dec-ae07-5599cf578deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"rec.motorcycles\", \"rec.sport.baseball\", \"sci.electronics\",\"sci.med\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b53121f-b992-439c-be6d-4e930368a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in newsgroups['data']:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1df4fd05-174f-4422-8310-aa3cb1a8d517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nrmendel',\n",
       "  'unix',\n",
       "  'amherst',\n",
       "  'edu',\n",
       "  'nathaniel',\n",
       "  'mendell',\n",
       "  'subject',\n",
       "  'bike',\n",
       "  'advice',\n",
       "  'organization',\n",
       "  'amherst',\n",
       "  'college',\n",
       "  'x',\n",
       "  'newsreader',\n",
       "  'tin',\n",
       "  'version',\n",
       "  'pl',\n",
       "  'lines',\n",
       "  'ummm',\n",
       "  'bikes',\n",
       "  'kx',\n",
       "  'suggest',\n",
       "  'look',\n",
       "  'zx',\n",
       "  'since',\n",
       "  'horsepower',\n",
       "  'whereas',\n",
       "  'might',\n",
       "  'bit',\n",
       "  'much',\n",
       "  'sincerely',\n",
       "  'nathaniel',\n",
       "  'zx',\n",
       "  'dod',\n",
       "  'ama'],\n",
       " ['grante',\n",
       "  'aquarius',\n",
       "  'rosemount',\n",
       "  'com',\n",
       "  'grant',\n",
       "  'edwards',\n",
       "  'subject',\n",
       "  'krillean',\n",
       "  'photography',\n",
       "  'reply',\n",
       "  'grante',\n",
       "  'aquarius',\n",
       "  'rosemount',\n",
       "  'com',\n",
       "  'grant',\n",
       "  'edwards',\n",
       "  'organization',\n",
       "  'rosemount',\n",
       "  'inc',\n",
       "  'lines',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'aquarius',\n",
       "  'stgprao',\n",
       "  'st',\n",
       "  'unocal',\n",
       "  'com',\n",
       "  'richard',\n",
       "  'ottolini',\n",
       "  'writes',\n",
       "  'living',\n",
       "  'things',\n",
       "  'maintain',\n",
       "  'small',\n",
       "  'electric',\n",
       "  'fields',\n",
       "  'enhance',\n",
       "  'certain',\n",
       "  'chemical',\n",
       "  'reactions',\n",
       "  'promote',\n",
       "  'communication',\n",
       "  'states',\n",
       "  'cell',\n",
       "  'communicate',\n",
       "  'cells',\n",
       "  'nervous',\n",
       "  'system',\n",
       "  'specialized',\n",
       "  'example',\n",
       "  'perhaps',\n",
       "  'uses',\n",
       "  'true',\n",
       "  'electric',\n",
       "  'fields',\n",
       "  'change',\n",
       "  'location',\n",
       "  'time',\n",
       "  'large',\n",
       "  'organism',\n",
       "  'also',\n",
       "  'true',\n",
       "  'special',\n",
       "  'photographic',\n",
       "  'techniques',\n",
       "  'applying',\n",
       "  'external',\n",
       "  'fields',\n",
       "  'kirillian',\n",
       "  'photography',\n",
       "  'interact',\n",
       "  'fields',\n",
       "  'resistances',\n",
       "  'caused',\n",
       "  'fields',\n",
       "  'make',\n",
       "  'interesting',\n",
       "  'pictures',\n",
       "  'really',\n",
       "  'kirlian',\n",
       "  'photography',\n",
       "  'taking',\n",
       "  'pictures',\n",
       "  'corona',\n",
       "  'discharge',\n",
       "  'objects',\n",
       "  'animate',\n",
       "  'inanimate',\n",
       "  'fields',\n",
       "  'applied',\n",
       "  'objects',\n",
       "  'millions',\n",
       "  'times',\n",
       "  'larger',\n",
       "  'biologically',\n",
       "  'created',\n",
       "  'fields',\n",
       "  'want',\n",
       "  'record',\n",
       "  'biologically',\n",
       "  'created',\n",
       "  'electric',\n",
       "  'fields',\n",
       "  'got',\n",
       "  'use',\n",
       "  'low',\n",
       "  'noise',\n",
       "  'high',\n",
       "  'gain',\n",
       "  'sensors',\n",
       "  'typical',\n",
       "  'eegs',\n",
       "  'ekgs',\n",
       "  'kirlian',\n",
       "  'photography',\n",
       "  'phun',\n",
       "  'physics',\n",
       "  'type',\n",
       "  'stuff',\n",
       "  'right',\n",
       "  'soaking',\n",
       "  'chunks',\n",
       "  'extra',\n",
       "  'fine',\n",
       "  'steel',\n",
       "  'wool',\n",
       "  'liquid',\n",
       "  'oxygen',\n",
       "  'hitting',\n",
       "  'hammer',\n",
       "  'like',\n",
       "  'kirlean',\n",
       "  'setup',\n",
       "  'fun',\n",
       "  'possibly',\n",
       "  'dangerous',\n",
       "  'perhaps',\n",
       "  'pictures',\n",
       "  'diagonistic',\n",
       "  'disease',\n",
       "  'problems',\n",
       "  'organisms',\n",
       "  'better',\n",
       "  'understood',\n",
       "  'perhaps',\n",
       "  'probably',\n",
       "  'grant',\n",
       "  'edwards',\n",
       "  'yow',\n",
       "  'vote',\n",
       "  'rosemount',\n",
       "  'inc',\n",
       "  'well',\n",
       "  'tapered',\n",
       "  'half',\n",
       "  'cocked',\n",
       "  'ill',\n",
       "  'conceived',\n",
       "  'grante',\n",
       "  'aquarius',\n",
       "  'rosemount',\n",
       "  'com',\n",
       "  'tax',\n",
       "  'deferred'],\n",
       " ['liny',\n",
       "  'sun',\n",
       "  'scri',\n",
       "  'fsu',\n",
       "  'edu',\n",
       "  'nemo',\n",
       "  'subject',\n",
       "  'bates',\n",
       "  'method',\n",
       "  'myopia',\n",
       "  'reply',\n",
       "  'lin',\n",
       "  'ray',\n",
       "  'met',\n",
       "  'fsu',\n",
       "  'edu',\n",
       "  'distribution',\n",
       "  'na',\n",
       "  'organization',\n",
       "  'scri',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'university',\n",
       "  'lines',\n",
       "  'bates',\n",
       "  'method',\n",
       "  'work',\n",
       "  'first',\n",
       "  'heard',\n",
       "  'newsgroup',\n",
       "  'several',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'got',\n",
       "  'hold',\n",
       "  'book',\n",
       "  'improve',\n",
       "  'sight',\n",
       "  'simple',\n",
       "  'daily',\n",
       "  'drills',\n",
       "  'relaxation',\n",
       "  'margaret',\n",
       "  'corbett',\n",
       "  'authorized',\n",
       "  'instructor',\n",
       "  'bates',\n",
       "  'method',\n",
       "  'published',\n",
       "  'talks',\n",
       "  'vision',\n",
       "  'improvement',\n",
       "  'relaxation',\n",
       "  'exercise',\n",
       "  'study',\n",
       "  'whether',\n",
       "  'method',\n",
       "  'actually',\n",
       "  'works',\n",
       "  'works',\n",
       "  'actually',\n",
       "  'shortening',\n",
       "  'previously',\n",
       "  'elongated',\n",
       "  'eyeball',\n",
       "  'increasing',\n",
       "  'lens',\n",
       "  'ability',\n",
       "  'flatten',\n",
       "  'order',\n",
       "  'compensate',\n",
       "  'long',\n",
       "  'eyeball',\n",
       "  'since',\n",
       "  'myopia',\n",
       "  'result',\n",
       "  'eyeball',\n",
       "  'elongation',\n",
       "  'seems',\n",
       "  'logical',\n",
       "  'approach',\n",
       "  'correction',\n",
       "  'find',\n",
       "  'way',\n",
       "  'reverse',\n",
       "  'process',\n",
       "  'e',\n",
       "  'shorten',\n",
       "  'somehow',\n",
       "  'preferably',\n",
       "  'non',\n",
       "  'surgically',\n",
       "  'recent',\n",
       "  'studies',\n",
       "  'find',\n",
       "  'know',\n",
       "  'rk',\n",
       "  'works',\n",
       "  'changing',\n",
       "  'curvature',\n",
       "  'cornea',\n",
       "  'compensate',\n",
       "  'shape',\n",
       "  'eyeball',\n",
       "  'way',\n",
       "  'train',\n",
       "  'muscles',\n",
       "  'shorten',\n",
       "  'eyeball',\n",
       "  'back',\n",
       "  'correct',\n",
       "  'length',\n",
       "  'would',\n",
       "  'even',\n",
       "  'better',\n",
       "  'bates',\n",
       "  'idea',\n",
       "  'right',\n",
       "  'thanks',\n",
       "  'information'],\n",
       " ['mcovingt',\n",
       "  'aisun',\n",
       "  'ai',\n",
       "  'uga',\n",
       "  'edu',\n",
       "  'michael',\n",
       "  'covington',\n",
       "  'subject',\n",
       "  'buy',\n",
       "  'parts',\n",
       "  'time',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'aisun',\n",
       "  'ai',\n",
       "  'uga',\n",
       "  'edu',\n",
       "  'organization',\n",
       "  'ai',\n",
       "  'programs',\n",
       "  'university',\n",
       "  'georgia',\n",
       "  'athens',\n",
       "  'lines',\n",
       "  'pricing',\n",
       "  'parts',\n",
       "  'reminds',\n",
       "  'something',\n",
       "  'chemist',\n",
       "  'said',\n",
       "  'gram',\n",
       "  'dye',\n",
       "  'costs',\n",
       "  'dollar',\n",
       "  'comes',\n",
       "  'liter',\n",
       "  'jar',\n",
       "  'also',\n",
       "  'costs',\n",
       "  'dollar',\n",
       "  'want',\n",
       "  'whole',\n",
       "  'barrel',\n",
       "  'also',\n",
       "  'costs',\n",
       "  'dollar',\n",
       "  'e',\n",
       "  'charge',\n",
       "  'almost',\n",
       "  'exclusively',\n",
       "  'packaging',\n",
       "  'delivering',\n",
       "  'chemical',\n",
       "  'particular',\n",
       "  'case',\n",
       "  'byproduct',\n",
       "  'cost',\n",
       "  'almost',\n",
       "  'nothing',\n",
       "  'intrinsically',\n",
       "  'michael',\n",
       "  'covington',\n",
       "  'associate',\n",
       "  'research',\n",
       "  'scientist',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'programs',\n",
       "  'mcovingt',\n",
       "  'ai',\n",
       "  'uga',\n",
       "  'edu',\n",
       "  'university',\n",
       "  'georgia',\n",
       "  'phone',\n",
       "  'athens',\n",
       "  'georgia',\n",
       "  'u',\n",
       "  'amateur',\n",
       "  'radio',\n",
       "  'n',\n",
       "  'tmi'],\n",
       " ['tammy',\n",
       "  'vandenboom',\n",
       "  'launchpad',\n",
       "  'unc',\n",
       "  'edu',\n",
       "  'tammy',\n",
       "  'vandenboom',\n",
       "  'subject',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'testicles',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'lambada',\n",
       "  'oit',\n",
       "  'unc',\n",
       "  'edu',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'north',\n",
       "  'carolina',\n",
       "  'extended',\n",
       "  'bulletin',\n",
       "  'board',\n",
       "  'service',\n",
       "  'distribution',\n",
       "  'na',\n",
       "  'lines',\n",
       "  'husband',\n",
       "  'woke',\n",
       "  'three',\n",
       "  'days',\n",
       "  'ago',\n",
       "  'small',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'spot',\n",
       "  'size',\n",
       "  'nickel',\n",
       "  'one',\n",
       "  'testicles',\n",
       "  'bottom',\n",
       "  'side',\n",
       "  'knots',\n",
       "  'lumps',\n",
       "  'little',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'says',\n",
       "  'reminds',\n",
       "  'bruise',\n",
       "  'feels',\n",
       "  'recollection',\n",
       "  'hitting',\n",
       "  'anything',\n",
       "  'like',\n",
       "  'would',\n",
       "  'cause',\n",
       "  'bruise',\n",
       "  'asssures',\n",
       "  'remember',\n",
       "  'something',\n",
       "  'like',\n",
       "  'clues',\n",
       "  'might',\n",
       "  'somewhat',\n",
       "  'hypochondriac',\n",
       "  'sp',\n",
       "  'sure',\n",
       "  'gonna',\n",
       "  'die',\n",
       "  'thanks',\n",
       "  'opinions',\n",
       "  'expressed',\n",
       "  'necessarily',\n",
       "  'university',\n",
       "  'north',\n",
       "  'carolina',\n",
       "  'chapel',\n",
       "  'hill',\n",
       "  'campus',\n",
       "  'office',\n",
       "  'information',\n",
       "  'technology',\n",
       "  'experimental',\n",
       "  'bulletin',\n",
       "  'board',\n",
       "  'service',\n",
       "  'internet',\n",
       "  'launchpad',\n",
       "  'unc',\n",
       "  'edu']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a99911f1-9b1e-4a36-98fd-366936a5b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 17.2 ms, total: 3.04 s\n",
      "Wall time: 924 ms\n"
     ]
    }
   ],
   "source": [
    "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0a4bc73-8c6b-4168-83fc-45b1eff4fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('want', 0.9860509634017944), ('work', 0.979192316532135), ('etc', 0.9710627794265747), ('cpu', 0.9693261384963989), ('using', 0.9661604762077332)]\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что модель обучилась\n",
    "print(model_imdb.wv.most_similar(positive=['find'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c7b2e92-5609-485d-9d37-0d55a72b57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_2(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f4f00-c58a-4d18-93a6-e615a1bf66b8",
   "metadata": {},
   "source": [
    "### Проверка качества работы модели word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f499768-dc7f-4eda-93e0-1fce5c1a1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb0654aa-6dbf-4427-a5b1-c6ca1fb29b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c2ba34b-1482-4672-8618-defa6878b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучающая и тестовая выборки\n",
    "boundary = 1500\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = newsgroups['target'][:boundary]\n",
    "y_test = newsgroups['target'][boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec2c8e65-a944-4168-ad4a-eece83b2ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8552631578947368\n",
      "1 \t 0.941747572815534\n",
      "2 \t 0.8165137614678899\n",
      "3 \t 0.7543859649122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wanhao/Downloads/anaconda/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "sentiment_2(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efa160-71b6-4d8a-aa31-a91e78188f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
